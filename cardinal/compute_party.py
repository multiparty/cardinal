import pystache
import os
import requests
import time
from cardinal.handlers.handler import Handler


class ComputeParty:
    def __init__(self, workflow_config: dict, app, handler: Handler):
        self.workflow_config = workflow_config
        self.app = app
        self.handler = handler
        self.templates_directory = f"{os.path.dirname(os.path.realpath(__file__))}/templates/compute_party/"
        self.specs = {}
        self.other_pod_ips = self._initialize_other_ips()
        self.compute_pod_ip = self.fetch_available_ip_address()

    def build_pod_spec(self):
        self.specs["POD"] = ""

    def build_service_spec(self):
        self.specs["SERVICE"] = ""

    def build_config_map(self):
        """
        The config map will have:
            - the congregation workflow to be run
            - the congregation config that gets generated by build_congregation_config()
            - the cloud storage endpoint for the input dataset (eg s3://datasets/input.csv)
                * we assume that the generated pod has environment variables injected into
                when we define the pod which allow it to authenticate against the cloud storage.
                These environment variables are also present on this machine (since we're making
                API calls to aws/gcloud/azure), so we can just grab them from the environment.
        """
        self.specs["CONFIG_MAP"] = ""

    def _initialize_other_ips(self):
        """
        we want a dictionary where each entry is like PID: <IP>:<PORT>, with a unique port for each
        compute party. The way I'm doing a unique port is just 9000 + PID, so like 9001, 9002, etc.

        I'm setting the ip:port entries for each other compute party to None, because we haven't
        actually exchanged IP addresses yet.
        """

        ret = {
            int(self.workflow_config["PID"]): f"{self.compute_pod_ip}:{9000 + int(self.workflow_config['PID'])}"
        }

        for party in self.workflow_config["other_cardinals"]:
            ret[int(party[0])] = None

        return ret

    def _check_ip_records(self):
        """
        Look at our record of other parties' compute pod IP addresses,
        return False if any of them are None, and return True if all
        are complete.
        """

        statuses = []
        for other_party in self.other_pod_ips.keys():
            statuses.append(self.other_pod_ips[other_party])

        return all(statuses)

    def _exchange_ips(self):
        """
        Just a dumb little protocol that continuously sends the IP address
        we fetched for the compute pod that will be generated to the other
        cardinal servers until our own table is complete.

        It would be much smarter to only send our fetched IP address to
        servers that haven't yet replied saying that they received it,
        and adding a GET API endpoint at wsgi.py that we could use to
        ask for an IP address, and have this loop use that instead of
        sending our own IP addresses. If this doesn't work right we can
        switch to that.
        """

        all_ips_received = False
        while not all_ips_received:

            for other_party in self.workflow_config["other_cardinals"]:

                req = {
                    "workflow_name": self.workflow_config["workflow_name"],
                    "from_pid": self.workflow_config["PID"],
                    "pod_ip_address": self.compute_pod_ip
                }

                dest_server = other_party[1]
                resp = requests.post(f"{dest_server}/submit_ip_address", json=req)
                self.app.logger(f"Submitted IP address to {dest_server} and got response: \n{resp}")

            if self._check_ip_records():
                all_ips_received = True
            else:
                self.app.logger.info("Waiting for IP addresses from some parties, trying again.")
                time.sleep(10)

    def _build_all_pids_list(self):

        ret = [int(self.workflow_config["PID"])]
        for party in self.workflow_config["other_cardinals"]:
            ret.append(party[0])

        return ret

    def _build_parties_config(self):
        """
        We call this method only after we've successfully run the self._exchange_ips()
        method, since it's input depend on that method's outputs.

        This function takes something like the following:
        {
            1: xx.xx.xx.xx:9001,
            2: yy.yy.yy.yy:9002,
            3: zz.zz.zz.zz:9003
        }

        And produces:
        ["1:xx.xx.xx.xx:9001", "2:yy.yy.yy.yy:9002", "3:zz.zz.zz.zz:9003"]

        Which is just the format that the congregation config file uses for IP addresses
        of the other compute parties.
        """
        return [f"{k}:{self.other_pod_ips[k]}" for k in self.other_pod_ips.keys()]

    def build_congregation_config(self):
        """
        Use pystache to build config from template files and data from workflow
        request, this flow can be used for building other specs.

        The parameters that will eventually be configurable will all have to do
        with the contents of the dataset. They're all jiff-specific and just have
        to do with what extensions/parameters were used to create the shares of
        the data.

        I think we can fetch this metadata using
        some kind of standard for how datasets are uploaded, like:

            - Say we have dataset input.csv on some s3 bucket called datasets.
            - we would then just ensure that when this dataset is uploaded at
            s3://datasets/input.csv, there's some other JSON file uploaded
            alongside it called input.json, that looks something like the following:

            {
                "use_floats": <bool>,
                "zp": <int>,
                "floating_point_use": <bool>,
                "floating_point_decimal": <int>,
                "floating_point_integer": <int>,
                "negative_number_use": <bool>,
                "bignumber_use": <bool>
            }

        And since we have the dataset ID from workflow_config, we can do a lookup
        for where that dataset is (lookup table OK to hardcode here for now), and
        then just grab a file from that location with a .json extension instead of
        .csv, since we ensure it exists when we upload the dataset.
        """

        template = open(f"{self.templates_directory}/congregation_config.tmpl").read()
        data = {
            "WORKFLOW_NAME": self.workflow_config["workflow_name"],
            "PID": int(self.workflow_config["PID"]),
            "ALL_PIDS": self._build_all_pids_list(),
            "USE_FLOATS": False,   # TODO - will eventually be configurable, hardcoded fine for now
            "PARTIES_CONFIG": self._build_parties_config(),
            "JIFF_SERVER_IP": self.workflow_config["jiff_server"].split(":")[0],
            "JIFF_SERVER_PORT": int(self.workflow_config["jiff_server"].split(":")[1]),
            "ZP": 16777729,        # TODO - will eventually be configurable, hardcoded fine for now
            "FP_USE": False,       # TODO - will eventually be configurable, hardcoded fine for now
            "FP_DECIMAL": 1,       # TODO - will eventually be configurable, hardcoded fine for now
            "FP_INTEGER": 1,       # TODO - will eventually be configurable, hardcoded fine for now
            "NN_USE": False,       # TODO - will eventually be configurable, hardcoded fine for now
            "BN_USE": False        # TODO - will eventually be configurable, hardcoded fine for now
        }

        rendered = pystache.render(template, data)
        self.specs["CONGREGATION_CONFIG"] = rendered

    def fetch_available_ip_address(self):
        return self.handler.fetch_available_ip_address()

    def launch_pod(self):

        if self.specs.get("POD") is None:
            self.app.logger.error("No pod spec defined.")
        self.handler.launch_pod(self.specs.get("POD"))

    def launch_service(self):

        if self.specs.get("SERVICE") is None:
            self.app.logger.error("No service spec defined.")
        self.handler.launch_service(self.specs.get("service"))

    def build_all(self):

        self.build_service_spec()
        self.build_pod_spec()
        self._exchange_ips()
        self.build_congregation_config()
        self.build_config_map()

    def launch_all(self):

        self.handler.launch_service(self.specs.get("SERVICE"))
        self.handler.launch_pod(self.specs.get("POD"))
